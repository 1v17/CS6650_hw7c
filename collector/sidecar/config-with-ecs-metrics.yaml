extensions:
  health_check:
    endpoint: "localhost:${OTEL_SIDECAR_HEALTH_PORT}"
    path: /health

receivers:
  awsecscontainermetrics:
    collection_interval: 30s
  otlp:
    protocols:
      grpc:
        endpoint: "localhost:${OTEL_SIDECAR_GRPC_PORT}"
      http:
        endpoint: "localhost:${OTEL_SIDECAR_HTTP_PORT}"

exporters:
  otlp:
    endpoint: "${OTEL_GATEWAY_GRPC_ENDPOINT}"
    sending_queue:
      queue_size: 2048
  otlphttp:
    endpoint: "${OTEL_GATEWAY_HTTP_ENDPOINT}"
    sending_queue:
      queue_size: 2048

processors:
  memory_limiter:
    check_interval: 1s
    limit_percentage: 80
    spike_limit_percentage: 15
  batch/traces:
    timeout: 1s
    send_batch_size: 50
  batch/metrics:
    timeout: 60s
  filter:
    # For the sake of simplicity, including a few metrics here.
    # In a real-world scenario, you would include all the metrics you want to export.
    # Available metrics: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awsecscontainermetricsreceiver/README.md#available-metrics
    metrics:
      include:
        match_type: strict
        metric_names:
          - ecs.task.memory.usage
          - ecs.task.memory.usage.max
          - ecs.task.memory.utilized
          - ecs.task.cpu.usage.total
          - ecs.task.cpu.utilized
          - ecs.task.cpu.usage.vcpu
          - ecs.task.cpu.usage.system

service:
  pipelines:
    metrics:
      receivers: [ otlp ]
      processors: [ memory_limiter, batch/metrics ]
      exporters: [ otlp, otlphttp ]
    metrics/container:
      receivers: [ awsecscontainermetrics ]
      processors: [ memory_limiter, filter, batch/metrics ]
      exporters: [ otlp, otlphttp ]
    traces:
      receivers: [ otlp ]
      processors: [ memory_limiter, batch/traces ]
      exporters: [ otlp, otlphttp ]
  extensions: [ health_check ]
  telemetry:
    logs:
      level: ${OTEL_SIDECAR_LOG_LEVEL}
      encoding: console
 